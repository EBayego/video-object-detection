{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2968d1f-f958-4647-97b1-e17bb44407c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e247cc1-c33a-48d5-a717-1ee240444855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 9\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Cambiar el tamaño de las imágenes según lo necesites\n",
    "        transforms.ToTensor(),  # Convertir la imagen a un tensor\n",
    "])\n",
    "img_data = ImageFolder(root=\"./images\", transform=transform)\n",
    "\n",
    "test_size = 0.15  \n",
    "num_train = int((1 - test_size) * len(img_data))\n",
    "num_test = len(img_data) - num_train\n",
    "\n",
    "train_data, test_data = random_split(img_data, [num_train, num_test])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=False)\n",
    "print(len(train_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444ef8ca-99ca-4e8f-a735-0ce9a1659a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura del modelo CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes, seed = 66):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "    \n",
    "        self.fc1 = nn.Linear(10*10*128, 512) #dimensions*output_features from the convolutional layer\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "    \n",
    "        x = x.view(x.size(0), -1) #flattening layer\n",
    "    \n",
    "        x = F.relu(self.fc1(x)) #sends the info throw the connected layers having the ReLU activation function applied\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    def predict_image(self, image_path):\n",
    "        image = transform(Image.open(image_path)).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self(image)\n",
    "\n",
    "        probabilities = torch.softmax(output, dim=1)[0]\n",
    "        predicted_label = torch.argmax(probabilities).item()\n",
    "\n",
    "        class_names = img_data.classes\n",
    "        predicted_class = class_names[predicted_label]\n",
    "\n",
    "        print(f'Predicted Class for {image_path}: {predicted_class}')\n",
    "        for i, prob in enumerate(probabilities):\n",
    "            print(f'{class_names[i]}: {prob:.4f}')\n",
    "\n",
    "    def predict_region(self, region_of_interest):\n",
    "        if isinstance(region_of_interest, np.ndarray):\n",
    "            region_of_interest = Image.fromarray(region_of_interest)\n",
    "        image = transform(region_of_interest).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self(image)\n",
    "\n",
    "        probabilities = torch.softmax(output, dim=1)[0]\n",
    "        predicted_label = torch.argmax(probabilities).item()\n",
    "\n",
    "        #class_names = img_data.classes\n",
    "        #predicted_class = class_names[predicted_label]\n",
    "\n",
    "        return predicted_label\n",
    "\n",
    "model = CNN(num_classes=len(img_data.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21da92f-18a9-4310-b7da-c701ca62d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18aff50-6563-4736-889b-5ead759f9535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.0825598680973052\n",
      "Epoch 2, Training Loss: 0.7298293957114219\n",
      "Epoch 3, Training Loss: 0.6778105515241623\n",
      "Epoch 4, Training Loss: 0.750079015493393\n",
      "Epoch 5, Training Loss: 0.6899474114179611\n",
      "Epoch 6, Training Loss: 0.5152839502692222\n",
      "Epoch 7, Training Loss: 0.46968883961439134\n",
      "Epoch 8, Training Loss: 0.4331125044822693\n",
      "Epoch 9, Training Loss: 0.44701874420046805\n",
      "Epoch 10, Training Loss: 0.37252117313444616\n",
      "Epoch 11, Training Loss: 0.3382280695438385\n",
      "Epoch 12, Training Loss: 0.35827051371335983\n",
      "Epoch 13, Training Loss: 0.3136134836636484\n",
      "Epoch 14, Training Loss: 0.1574496181681752\n",
      "Epoch 15, Training Loss: 0.11455472772242502\n",
      "Epoch 16, Training Loss: 0.10226235596288462\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "epochs = 16\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c9041cb-b2c8-4d1f-a0d5-11fdbeb0c450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 73.2394366197183%\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo en el conjunto de prueba\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "096c1dfb-c034-40ba-a801-d145f770cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class for ./images/keys/llaves_17.jpg: keys\n",
      "keys: 0.9576\n",
      "pencil: 0.0345\n",
      "sneakers: 0.0080\n"
     ]
    }
   ],
   "source": [
    "model.predict_image(\"./images/keys/llaves_17.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea500432-1798-4b6f-98a3-a61df3050846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bounding_boxes(image):\n",
    "    # Convertir la imagen a escala de grises\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Aplicar un umbral para obtener una imagen binaria\n",
    "    _, thresh = cv2.threshold(gray, 147, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Aplicar erosión y dilatación para eliminar ruido y conectar regiones\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Encontrar contornos en la imagen binaria\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    bounding_boxes = []\n",
    "    \n",
    "    # Iterar sobre los contornos encontrados\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > 50:  # Filtrar contornos con un área mínima de 100 píxeles\n",
    "            # Obtener las coordenadas de la caja delimitadora que rodea al contorno\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            bounding_boxes.append((x, y, x+w, y+h))  # Formato de caja: (x_min, y_min, x_max, y_max)\n",
    "    \n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbba428f-78c0-4144-bdca-3106364b2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(images, images_names): #Generar KEYS\n",
    "    annotations = []\n",
    "    categories = [\n",
    "        {\"id\": 91, \"name\": \"keys\"},\n",
    "        {\"id\": 92, \"name\": \"pencil\"},\n",
    "        {\"id\": 93, \"name\": \"sneakers\"}\n",
    "    ]\n",
    "    \n",
    "    for idx, image in enumerate(images):\n",
    "        bounding_boxes = generate_bounding_boxes(image)\n",
    "        \n",
    "        for bbox in bounding_boxes:\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            region_of_interest = image[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "            #category_id = model.predict_region(region_of_interest) + 91 # Coco dataset ends in id=90  \n",
    "            category_id = 91\n",
    "            annotations.append({\"image_id\": idx, \"category_id\": category_id, \"bbox\": [x_min, y_min, x_max - x_min, y_max - y_min]})\n",
    "    \n",
    "    # Construir el diccionario JSON final\n",
    "    json_data = {\"annotations\": annotations, \"categories\": categories, \"images\": [{\"id\": i, \"file_name\": images_names[i]} for i in range(len(images_names))]}\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ccc1004-2b26-4c0a-b90a-10a150f48793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_images(images, images_names, category=0):\n",
    "    annotations = []\n",
    "    categories = [\n",
    "        {\"id\": 91, \"name\": \"keys\"},\n",
    "        {\"id\": 92, \"name\": \"pencil\"},\n",
    "        {\"id\": 93, \"name\": \"sneakers\"}\n",
    "    ]\n",
    "    \n",
    "    for idx, image in enumerate(images):\n",
    "        bounding_boxes = generate_bounding_boxes(image)\n",
    "        \n",
    "        for bbox in bounding_boxes:\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            #region_of_interest = image[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "            #category_id = model.predict_region(region_of_interest) + 91 # Coco dataset ends in id=90  \n",
    "            #category_id = 91\n",
    "            if \"llaves\" in images_names[idx]:\n",
    "                category_id = 91\n",
    "            elif \"pencil\" in images_names[idx]:\n",
    "                category_id = 92\n",
    "            elif \"zapatilas\" in images_names[idx]:\n",
    "                category_id = 93\n",
    "            else:\n",
    "                category_id = 0  # Otra categoría por defecto\n",
    "            annotations.append({\"image_id\": idx, \"category_id\": category_id, \"bbox\": [x_min, x_max, y_min, y_max]})\n",
    "    \n",
    "    # Construir el diccionario JSON final\n",
    "    json_data = {\"annotations\": annotations, \"categories\": categories, \"images\": [{\"id\": i, \"file_name\": images_names[i]} for i in range(len(images_names))]}\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ebc8180f-a2b3-4598-a0b6-26cfdd75a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAR BOXES A UNA IMAGEN\n",
    "image = cv2.imread('./images/keys/llaves_142.jpg')\n",
    "\n",
    "# Generar las cajas delimitadoras\n",
    "bounding_boxes = generate_bounding_boxes(image)\n",
    "\n",
    "# Dibujar las cajas delimitadoras en la imagen original\n",
    "for box in bounding_boxes:\n",
    "    cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "# Mostrar la imagen con las cajas delimitadoras\n",
    "cv2.imshow('Bounding Boxes', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a39fe775-792d-4f16-9861-7e36edde8e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations saved to annotations.json\n"
     ]
    }
   ],
   "source": [
    "# GENERAR BOXES A TODA LA CARPETA DE IMAGENES DE KEYS\n",
    "images = []\n",
    "images_names = []\n",
    "\n",
    "# Obtener la lista de nombres de archivos de imagen en la carpeta\n",
    "image_files = glob.glob(\"./images/keys/*.jpg\") \n",
    "image_files = sorted(image_files, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "# Iterar sobre cada archivo de imagen y cargarlo\n",
    "for image_file in image_files:\n",
    "    images_names.append(image_file.split('\\\\')[-1])\n",
    "    image = cv2.imread(image_file)\n",
    "    if image is not None:\n",
    "        images.append(image)\n",
    "    else:\n",
    "        print(f\"Error: No se pudo cargar la imagen {image_file}\")\n",
    "\n",
    "# Procesar las imágenes y generar las anotaciones\n",
    "annotations_data = process_images(images, images_names)\n",
    "\n",
    "# Guardar los datos en un archivo JSON\n",
    "with open(\"annotations.json\", \"w\") as json_file:\n",
    "    json.dump(annotations_data, json_file, indent=4)\n",
    "\n",
    "print(\"Annotations saved to annotations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f74d148-5b17-4389-ad71-3186a49ee22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sort_key(file_path):\n",
    "    # Obtener el nombre del archivo sin la extensión\n",
    "    file_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    # Separar el nombre del archivo y el número al final\n",
    "    name, number = re.match(r'([^_]+)_(\\d+)', file_name).groups()\n",
    "    \n",
    "    return name, int(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6169a27e-5299-430c-baab-7b3f53b4f5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations saved to annotations.json\n"
     ]
    }
   ],
   "source": [
    "# GENERAR BOXES A TODAS LA CARPETA DE IMAGENES\n",
    "images = []\n",
    "images_names = []\n",
    "\n",
    "# Obtener la lista de nombres de archivos de imagen en la carpeta\n",
    "image_files = glob.glob(\"./images/images/train/*.jpg\") \n",
    "image_files = sorted(image_files, key=custom_sort_key)\n",
    "\n",
    "# Iterar sobre cada archivo de imagen y cargarlo\n",
    "for image_file in image_files:\n",
    "    images_names.append(image_file.split('\\\\')[-1])\n",
    "    image = cv2.imread(image_file)\n",
    "    if image is not None:\n",
    "        images.append(image)\n",
    "    else:\n",
    "        print(f\"Error: No se pudo cargar la imagen {image_file}\")\n",
    "\n",
    "# Procesar las imágenes y generar las anotaciones\n",
    "annotations_data = process_all_images(images, images_names)\n",
    "\n",
    "# Guardar los datos en un archivo JSON\n",
    "with open(\"all_annotations.json\", \"w\") as json_file:\n",
    "    json.dump(annotations_data, json_file, indent=4)\n",
    "\n",
    "print(\"Annotations saved to annotations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349bbd3-7a84-4b95-9eeb-a2939db655a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
